Prerequisites:
==============
0. Ubuntu 16.04 should be used as host OS for build.
1. Install tools : apt-get install gawk wget diffstat texinfo chrpath socat libsdl1.2-dev \
                    python-crypto repo checkpolicy python-git python-github \
                    python-ctypeslib bzr pigz m4 lftp openjdk-8-jdk git-core \
                    gnupg flex bison gperf build-essential zip curl zlib1g-dev \
                    gcc-multilib g++-multilib libc6-dev-i386 lib32ncurses5-dev \
                    x11proto-core-dev libx11-dev lib32z-dev ccache libgl1-mesa-dev \
                    libxml2-utils xsltproc unzip python-clang-5.0 gcc-5 g++-5 bc \
                    python3-pyelftools python3-pip gcc-aarch64-linux-gnu python3-crypto -y
2. Install package pycryptodome: sudo pip3 install pycryptodomex 
3. Download and install Google repo: https://source.android.com/setup/build/downloading#installing-repo
4. Checked with Python v 2.7.12, but other should also work

About:
======
Prod-devel product is based on Renesas BSP and Xen hypervisor. It contains a set of features required to have
both Display and GPU support in several domains at the same time (device passthrough, PV Display, PV Sound, VGPU, etc).

There are three domains which are running on top of Xen:
1. Generic machine independent control domain named "Domain-0". This initramfs based domain is responsible
for managing VMs only (create/destroy/reboot guest domains). There is no HW assigned to this domain.
2. Machine dependent driver domain named "DomD". Almost all HW are assigned to this domain.
As this domain is not 1:1 mapped, Xen controls IPMMU to do a proper address translations (IPA to PA) for DMA masters.
It also contains different para-virtualized (PV) backends running inside it in order to provide guest domains
(without HW assigned) with different services such as Audio, Display, Net, Block, etc.
3. Linux based domain named "DomU".
It has different PV frontends running and doesn't have any HW assigned except GPU.
The latter is shared between "DomD" and "DomU" in the current setup.

Build:
======
Our build system uses set of additional meta layers and tools.
1. Please, clone the following build scripts, master branch:

git clone https://github.com/xen-troops/build-scripts.git
cd build-scripts

2. In the build-scripts directory you will find a sample configuration
file gcp-build-server-devel.cfg:

cp gcp-build-server-devel.cfg xt-prod-devel.cfg

3. Edit it to fit your environment.

3.1 Please, change the variable under [git] section:

xt_manifest_uri = https://github.com/xen-troops/meta-xt-products.git

3.2 Please, change the variables under [path] section:

 - workspace_base_dir: change it to point to the place where the build will happen
 - workspace_storage_base_dir: change it to where downloads and other files will be put
 - workspace_cache_base_dir: it points to state cache

For example,

workspace_base_dir = /home/workspace_base
workspace_storage_base_dir = /home/workspace_storage_base
workspace_cache_base_dir = /home/workspace_cache

3.3 Please, change the variables under [local_conf] section:

3.3.1 The XT_GUESTS_INSTALL and XT_GUESTS_BUILD variables select the guest domain
to be built and installed.

XT_GUESTS_INSTALL = "domu"
XT_GUESTS_BUILD = "domu"

3.3.2 The XT_RCAR_EVAPROPRIETARY_DIR (DomD and DomU) variables
point to the place where prebuilt EVA proprietary "graphics" packages (hereafter "graphics" packages) will be copied in step #4.

If you are going to build Graphics DDK from sources (and don't use "graphics" packages), please remove those variables.
In that case step #4 can be omitted.

3.3.3 The XT_RCAR_PROPRIETARY_MULTIMEDIA_DIR (DomD) variable points to the place where proprietary "multimedia" packages
(hereafter "multimedia" packages) will be copied in step #5.

If you are not going to use multimedia in DomD, please remove that variable. In that case step #5 can be omitted.

4. Download and copy "graphics" packages to your local filesystem at some directory:

4.1 DomD and DomU prebuilts:

For Salvator-X board with H3 ES3.0 SoC installed, 8GB RAM:

 - rcar-proprietary-graphic-salvator-x-h3-4x2g-xt-domd.tar.gz
 - rcar-proprietary-graphic-salvator-x-h3-4x2g-xt-domu.tar.gz

Copy it to "..PROPRIETARY_FOLDER_PATH.." folder.

XT_RCAR_EVAPROPRIETARY_DIR should point to "..PROPRIETARY_FOLDER_PATH.." folder.

5. Only for the multimedia usage.
Follow the procedure described in "Preliminary steps #1" from https://elinux.org/R-Car/Boards/Yocto-Gen3
to download Multimedia and Graphics library and related Linux drivers. These software packages
should be stored at the directory pointed by XT_RCAR_PROPRIETARY_MULTIMEDIA_DIR variable.

Please note, although only "multimedia" packages will be used here, there is no need
to re-pack package archives in order to remove "graphics" packages, unneeded packages
will be just skipped.

6. Run the build script for current stable release:

python ./build_prod.py --build-type dailybuild --machine salvator-x-h3-4x2g --product PRODUCT_NAME \
--branch pci_phase2 --with-local-conf --config xt-prod-devel.cfg --with-do-build

Where the PRODUCT_NAME depends on whether "graphics" packages are used or not:
- devel (to use "graphics" packages)
- devel-src (to build Graphics DDK from sources)

7. You are done. The artifacts of the build are located at workspace_base directory:

workspace_base/build/build/deploy/
├── dom0-image-thin-initramfs
│   └── images
│       └── generic-armv8-xt
│
├── domd-image-weston
│   └── images
│       └──  MACHINE_NAME-xt
│
├── domu-image-weston
│   └── images
│       └── MACHINE_NAME-xt
│
└── mk_sdcard_image.sh


Images are located at:

- Domain-0:
workspace_base/build/build/deploy/dom0-image-thin-initramfs/images/generic-armv8-xt
Here we get a part of boot images:
  - uInitramfs - thin-initramfs for Domain-0
  - Image - Kernel image for Domain-0

- DomD:
workspace_base/build/build/deploy/domd-image-weston/images/MACHINE-NAME-xt
Here we get a part of boot images, all bootloader images and rootfs image for DomD:
  - xen-uImage - Xen main image
  - xenpolicy - special image for Xen usage
  - dom0.dtb - device-tree image for Domain-0
  - bootloader images in both binary and srec formats.
  - core-image-weston-MACHINE_NAME-xt.tar.bz2 - rootfs image for DomD

- DomU:
workspace_base/build/build/deploy/domu-image-weston/images/MACHINE-NAME-xt
Here we get a rootfs image for DomU:
  - core-image-weston-MACHINE_NAME-xt.tar.bz2 - rootfs image for DomU

Build logs are located at:
- Domain-0:
workspace_base/build/build/log/dom0-image-thin-initramfs/cooker/generic-armv8-xt
- DomD:
workspace_base/build/build/log/domd-image-weston/cooker/MACHINE-NAME-xt
- DomU:
workspace_base/build/build/log/domu-image-weston/cooker/MACHINE-NAME-xt

8. If one wants to build any domain's images by hand, at the time of development
for instance, it is possible by going into desired directory and using poky to build:

- For building Domain-0:
cd workspace_base/build/build/tmp/work/x86_64-xt-linux/dom0-image-thin-initramfs/1.0-r0/repo/

- For building DomD:
cd workspace_base/build/build/tmp/work/x86_64-xt-linux/domd-image-weston/1.0-r0/repo/

- For building DomU:
cd workspace_base/build/build/tmp/work/x86_64-xt-linux/domu-image-weston/1.0-r0/repo/

source poky/oe-init-build-env

- For building Domain-0:
bitbake core-image-thin-initramfs

- For building DomD or DomU:
bitbake core-image-weston

QEMU
====
If basic emulation level is needed it is possible to run a limited setup with QEMU. In this mode only two guests are built: Domain-0 and DomU, no DomD.
In this setup Domain-0 has all required software to run without DomD.
The limitations of such a system are at least the following:
- Because QEMU doesn't support ITS it is not possible to provide MSI/MSI-X functionality to DomU.
- QEMU has a very limited support for Intel Corporation 82576 Gigabit Network Connection adapter with SR-IOV capability, so it is not possible to fully use it even without Xen and PCI passthrough.

In case of QEMU is built the deploy directory will contain an additional folder with the QEMU run script and its configurations.

workspace_base/build/build/deploy/
├── emu
│   ├── e1000e_single.cfg
│   ├── igb_single.cfg
│   ├── qemu-run-zynqmp-xilinx-xen.sh
│   ├── rtl8139_dual.cfg
│   ├── rtl8139_single.cfg

1. Building QEMU flavor of the release is the same process as above, but with a little addition.
1.1. In the configuration file of the build in the "[local_conf]" section please add:
XT_COMMON_DISTRO_FEATURES_APPEND = "qemu_xen"
1.2. Build as described above in the Build section.

2. Configuring and running QEMU + Xen setup.
2.1. In order to run QEMU there is a shell script and number of its configuration files provided.

2.2. Emulating SR-IOV with Intel Corporation 82576 Gigabit Network Connection

2.2.1. Current implementation contains a limited pre-view of SR-IOV support in Xen.
In order to run the demo use igb_single.cfg:
cd workspace_base/build/build/deploy/
./qemu-run-zynqmp-xilinx-xen.sh --config igb_single.cfg

2.2.2. After Domain-0 is up insert a Linux kernel module for the 82576 card:
modprobe igb max_vfs=4

You should see 4 virtual functions added:

root@generic-armv8-xt-dom0:~# lspci
00:00.0 PCI bridge: Xilinx Corporation Device d022 (rev 01)
01:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
01:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
01:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
01:10.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
01:10.6 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

2.2.3. Edit DomU configuration file and select proper PCI devices to be passed through:

root@generic-armv8-xt-dom0:~# vi /xt/dom.cfg/domu.cfg

and comment all the lines starting with "pci=" and enable Intel 82576 only:

# Intel Corporation 82576 Gigabit Network Connection
pci=["01:10.0,seize=1", "01:10.4,seize=1"]

Save the file.

2.2.4. Run DomU as usually:
root@generic-armv8-xt-dom0:~# xl create -c /xt/dom.cfg/domu.cfg

2.2.5. During the domain boot you should see something like
[    9.367877] igbvf: Intel(R) Gigabit Virtual Function Network Driver - version 2.4.0-k
[    9.378262] igbvf: Copyright (c) 2009 - 2012 Intel Corporation.
[    9.423144] igbvf 0000:01:00.0: enabling device (0000 -> 0002)
[    9.457897] igbvf 0000:01:00.0: Failed to initialize MSI-X interrupts.
[   12.328589] igbvf 0000:01:00.0: PF still in reset state. Is the PF interface up?
[   12.335281] igbvf 0000:01:00.0: Assigning random MAC address.
[   13.789139] igbvf 0000:01:00.0: PF still resetting
[   13.826405] igbvf 0000:01:00.0: Intel(R) 82576 Virtual Function
[   13.832294] igbvf 0000:01:00.0: Address: a2:8c:a2:90:77:ad
[   53.710012] igbvf 0000:01:01.0 enp1s1: renamed from eth1

2.2.6. List PCI devices passed through to the guest:
root@salvator-x-h3-4x2g-xt-domu:~# lspci
00:00.0 PCI bridge: XenSource, Inc. Device c300
01:00.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
01:01.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

2.2.7. As QEMU has limited support for Intel 82576 and there is no currently
MSI/MSI-X support for Xen you won't be able to use the passed through device
in full though.

2.3. Emulating dual RTL8139 PCI Fast Ethernet Adapters (limited, no MSI)

This setup demonstrates the use of PCI functions with non-page-aligned memory spaces:
for PCI passthrough to work there is a requirement that those are always page aligned.

2.3.1. In order to run the demo use rtl8139_dual.cfg:
cd workspace_base/build/build/deploy/
./qemu-run-zynqmp-xilinx-xen.sh --config rtl8139_dual.cfg

2.3.2. Edit DomU configuration file
vi /xt/dom.cfg/domu.cfg
and comment all "pci=" entries and uncomment the one below "Dual RTL8139 setup":
pci=["02:02.0,seize=1", "02:03.0,seize=1"]

Comment all "extra=" entries and uncomment the one below "Align MMIO for dual RTL8139 setup":
extra = "root=/dev/xvda1 rw rootwait console=hvc0 pci=resource_alignment=01:00.0;01:01.0"

2.3.3. Re-scan PCI devices in Domain-0 and force Linux to re-allign RTL devices:
root@generic-armv8-xt-dom0:~# /xt/scripts/rtl8139_align_mmio.sh

Check that RTL adapters now have their MMIOs page aligned:

root@generic-armv8-xt-dom0:~# lspci -v
...
02:02.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL-8100/8101L/8139 PCI Fast Ethernet Adapter (rev 20)
...
	Memory at e0000000 (32-bit, non-prefetchable) [disabled] [size=4K]
...

02:03.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL-8100/8101L/8139 PCI Fast Ethernet Adapter (rev 20)
...
	Memory at e0001000 (32-bit, non-prefetchable) [disabled] [size=4K]
...

2.3.4. Run DomU as usual.

2.3.5. Insert RTL Linux kernel module:
root@salvator-x-h3-4x2g-xt-domu:~# modprobe 8139cp

2.3.6. Check the list of PCI devices with lspci.


Usage:
======
It is possible to run system either using TFTP boot with NFS root or keeping
all images on a storage device such as eMMC or SD card.

Different helpers scripts and docs are located at:
build-workspace/build/meta-xt-prod-devel/doc

Let's consider available boot options in details.

1. Using a storage device.
In order to boot system using a storage device, required storage device should be prepared
and flashed beforehand. The mk_sdcard_image.sh script is intended to help with that:

sudo ./mk_sdcard_image.sh -p /IMAGE_FOLDER -d /IMAGE_FILE -c devel

Where, IMAGE_FOLDER is a path to a folder where artifacts live (in the context of this document
it is a "deploy" directory) and IMAGE_FILE is an output image file or physical device how
it is appears in the filesystem (/dev/sdx). As far as script intended to support different products,
we need to specify '-c devel' for this product.

1.1 In case of SDx card booting we just have to insert SD card to a Host machine and run a script,
the latter will do all required actions automatically. All what we need to care about is to write
proper environment variables from U-Boot command line (boot_dev/bootcmd) according to the chosen SDx.
See workspace_base/meta-xt-prod-gen3-test/doc/u-boot-env-salvator-x.txt for details.

1.2 In case of eMMC booting, we have to have an access to it in order to flash images.
It is going to be not quite easy as for removable SD card, but the one of the possible ways is
to prepare the image blob using the same script, copy resulting blob to NFS root directory,
set system to boot via NFS, go to a DomD on target (where eMMC device is available)
and using "dd" command just copy blob to eMMC.

For example,

prepare an image blob:

sudo ./mk_sdcard_image.sh -p /IMAGE_FOLDER -d /home/emmc.img -c gen3

and then run on target:
dd if=/home/emmc.img of=/dev/mmcblk0

After getting eMMC flashed we have to choose it to be an boot device in a similar way as it is done
for SD card.
See u-boot-env.txt for details.

2. Using TFTP boot with NFS root.
In case of TFTP booting we have to copy the following boot images into target-accessible TFTP boot
directory and extract guest domain rootfs images for being an NFS root directories for guest domains.
See https://github.com/xen-troops/manifests/wiki on how to setup TFTP, NFS, etc.

For example:

copy boot images to TFTP boot directory:
sudo mkdir /srv
cd workspace_base/build/build/tmp/deploy/dom0-image-thin-initramfs/images/generic-armv8-xt/
sudo cp uInitramfs /srv/
sudo cp Image /srv/
cd workspace_base/build/build/tmp/deploy/domd-image-weston/images/MACHINE_NAME-xt/
sudo cp dom0.dtb /srv/
sudo cp xenpolicy /srv/
sudo cp xen-uImage /srv/

extract DomD rootfs images:
sudo mkdir /srv/domd
cd workspace_base/build/build/tmp/deploy/domd-image-weston/images/MACHINE_NAME-xt/
sudo tar -xf core-image-weston-MACHINE_NAME-xt.tar.bz2 -C /srv/domd/

extract DomU rootfs images:
sudo mkdir /srv/domu
cd workspace_base/build/build/tmp/deploy/domu-image-weston/images/MACHINE_NAME-xt/
sudo tar -xf core-image-weston-MACHINE_NAME-xt.tar.bz2 -C /srv/domu/

It worth mentioning that some changes to domain config files are necessary to switch between NFS
and any of storage devices.

There are two options for doing that. Either by modifying source files followed by rebuilding
the whole Domain-0 or by modifying domain config files right in a built uInitramfs image which
implies unpacking an image before and packing it back after (an "uirfs.sh" script which
we will consider down the document is intended to help with that).

In order to modify sources go to the files location in inner yocto:
cd workspace_base/build/build/tmp/work/x86_64-xt-linux/dom0-image-thin-initramfs/1.0-r0/repo/meta-xt-prod-extra/recipes-extended/guest-addons/files

- Edit domd-MACHINE_NAME.cfg to uncomment "extra" option which corresponds to NFS.
- Edit domu-MACHINE_NAME.cfg to uncomment "extra" option which corresponds to NFS and comment "disk" option.

and then rebuild Domain-0 manually:
cd workspace_base/build/build/tmp/work/x86_64-xt-linux/dom0-image-thin-initramfs/1.0-r0/repo/
source poky/oe-init-build-env
bitbake core-image-thin-initramfs

So, as you can see, varying U-Boot's "boot_dev" and "bootcmd" environment variables
and domain config's "extra" and "disk" options it is possible to choose  different boot device
for each system component. For example, boot system using TFTP and then use SD card for guest domains.

Additional script available in the product.
1. uirfs.sh.
This script is intended to pack/unpack uInitramfs for Domain-0. It might be helpful since uInitramfs
contains a lot of things which may changed during testing. The "xt" directory ships all guest domain
configs, device-tree and Kernel images, etc.

For example,

unpack uInitramfs:
cd /srv
sudo mkdir initramfs
sudo ./uirfs.sh unpack uInitramfs initramfs

Modify it's components if needed.
For example, domain config files located at:
/srv/initramfs/xt/dom.cfg/

pack it back:
sudo ./uirfs.sh pack uInitramfs initramfs
```
