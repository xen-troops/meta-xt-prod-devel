Prerequisites:
==============
0. Ubuntu 16.04 should be used as host OS for build.
1. Install tools : apt-get install gawk wget diffstat texinfo chrpath socat libsdl1.2-dev \
                    python-crypto repo checkpolicy python-git \
                    python-ctypeslib bzr pigz m4 lftp openjdk-8-jdk git-core \
                    gnupg flex bison gperf build-essential zip curl zlib1g-dev \
                    gcc-multilib g++-multilib libc6-dev-i386 lib32ncurses5-dev \
                    x11proto-core-dev libx11-dev lib32z-dev ccache libgl1-mesa-dev \
                    libxml2-utils xsltproc unzip python-clang-5.0 -y
2. Download and install Google repo: https://source.android.com/setup/build/downloading#installing-repo
3. Checked with Python v 2.7.12, but other should also work

About:
======
Prod-devel product is based on Renesas BSP, Xen hypervisor and Android. It contains a set of features required to have
both Display and GPU support in several domains at the same time (device passthrough, PVDRM, VGPU, etc).

There are three domains which are running on top of Xen:
1. Generic machine independent control domain named "Domain-0". This initramfs based domain is responsible
for managing VMs only (create/destroy/reboot guest domains). There is no HW assigned to this domain.
2. Machine dependent driver domain named "DomD". Almost all HW are assigned to this domain.
As this domain is not 1:1 mapped, Xen controls IPMMU to do a proper address translations (IPA to PA) for DMA masters.
It also contains different para-virtualized (PV) backends running inside it in order to provide guest domains
(without HW assigned) with different services such as Audio, Display, Net, Block, etc.
3. Android based domain named "DomA". It has different PV frontends running and doesn't have any HW assigned except GPU.
The latter is shared between "DomD" and "DomA" in current setup. 

It worth to mention that IPMMU for GPU is enabled in case of H3 ES3.0 only. For other supported SoCs the PVRKM
performs IPA to PA translations using a hypercall to hypervisor.

The ARM TF was patched to bring up all available CPU cores.

Build:
======
Our build system uses set of additional meta layers and tools.
1. Please, clone the following build scripts, master branch:

git clone https://github.com/xen-troops/build-scripts.git
cd build-scripts

2. In the build-scripts directory you will find a sample configuration
file gcp-build-server-devel.cfg:

cp gcp-build-server-devel.cfg xt-prod-devel.cfg

3. Edit it to fit your environment.

Please, change the variable under [git] section:

xt_manifest_uri = https://github.com/xen-troops/meta-xt-products.git

Please, change the variables under [path] section:
 - workspace_base_dir: change it to point to the place where the build will happen
 - workspace_storage_base_dir: change it to where downloads and other files will be put
 - workspace_cache_base_dir: it points to state cache

For example,

workspace_base_dir = /home/workspace_base
workspace_storage_base_dir = /home/workspace_storage_base
workspace_cache_base_dir = /home/workspace_cache

The XT_RCAR_EVAPROPRIETARY_DIR, XT_ANDROID_PREBUILDS_DIR variables under [local_conf] section depends
on whether prebuilt EVA proprietary packages are going to be used.
If you are going to use prebuilt EVA proprietary packages, please change XT_RCAR_EVAPROPRIETARY_DIR, XT_ANDROID_PREBUILDS_DIR
variables to point to the place where EVA proprietary packages will be copied in step #4.
If you are going to build Graphics DDK from sources (without prebuilt EVA proprietary packages), please remove that variables.
In that case step #4 can be omitted.

4. Download and copy EVA proprietary packages to your local filesystem at some directory:

DomA prebuilts:

 - rcar-prebuilts-salvator-xs-h3-xt-doma.tar.gz

Unpack it to some "..PREBUILTS_FOLDER_PATH.."
I.e. you should have such directories structure:
"..ANDROID_PREBUILTS_FOLDER_PATH.."
├── pvr-km
│   └── pvrsrvkm.ko
└── pvr-um
    ├── data
    ├── prebuilds.mk
    ├── system
    └── vendor
        ├── bin
        │   ├── hwperfbin2jsont
        │   ├── pvrhwperf
        │   ├── pvrlogdump
        │   ├── pvrlogsplit
        │   └── pvrsrvctl
        ├── etc
        │   └── firmware
        │       ├── rgx.fw.signed.4.46.6.62
        │       └── rgx.fw.signed.4.46.6.62.vz
        ├── lib
        │   ├── egl
        │   │   ├── libEGL_POWERVR_ROGUE.so
        │   │   ├── libGLESv1_CM_POWERVR_ROGUE.so
        │   │   └── libGLESv2_POWERVR_ROGUE.so
        │   ├── hw
        │   │   ├── gralloc.r8a7795.so
        │   │   └── memtrack.r8a7795.so
        │   ├── libglslcompiler.so
        │   ├── libIMGegl.so
        │   ├── libPVRScopeServices.so
        │   ├── libsrv_um.so
        │   ├── libtqvalidate.so
        │   └── libusc.so
        └── lib64
            ├── egl
            │   ├── libEGL_POWERVR_ROGUE.so
            │   ├── libGLESv1_CM_POWERVR_ROGUE.so
            │   └── libGLESv2_POWERVR_ROGUE.so
            ├── hw
            │   ├── gralloc.r8a7795.so
            │   └── memtrack.r8a7795.so
            ├── libglslcompiler.so
            ├── libIMGegl.so
            ├── libPVRScopeServices.so
            ├── libsrv_um.so
            ├── libtqvalidate.so
            └── libusc.so


XT_ANDROID_PREBUILDS_DIR should points to "..ANDROID_PREBUILTS_FOLDER_PATH.." folder.

DomD prebuilts:

- rcar-proprietary-graphic-salvator-x-h3-4x2g-xt-domd.tar.gz

Copy it to "..PROPRIETARY_FOLDER_PATH.." folder.

XT_RCAR_EVAPROPRIETARY_DIR should points to "..PROPRIETARY_FOLDER_PATH.." folder.

Due to the fact that the SoC is not actually changed for Salvator-XS board,
you could reuse already provided binaries by making symlinks to corresponding archives.

For example (Salvator-XS case),

ln -s rcar-proprietary-graphic-salvator-x-h3-4x2g-xt-domd.tar.gz rcar-proprietary-graphic-salvator-xs-h3-4x2g-xt-domd.tar.gz

AOS VIS prebuilts:

AOS_VIS_PACKAGE_DIR should points to the directory with: aos-vis_git-r0_aarch64.ipk, ca-certificates_20170717-r0_all.ipk


5. Run the build script for current stable release:

python ./build_prod.py --build-type dailybuild --machine MACHINE_NAME --product PRODUCT_NAME --branch RELEASE_NAME --with-local-conf --config xt-prod-devel.cfg --with-do-build

Where the PRODUCT_NAME depends on whether prebuilt EVA proprietary packages are used or not:
- devel (to use prebuilt EVA proprietary packages)
- devel-src (to build Graphics DDK from sources)

Where the RELEASE_NAME name is the name of the release to be built.
This is done in order to track product releases easily and have a possibility to build any release just by pointing a proper release name.
Please note, if "branch" parameter is not set then the current product's snapshot will be built.

Hereafter the supported MACHINE_NAMEs are:
- salvator-x-h3  (Salvator-X board with H3 ES2.0 SoC installed)
- salvator-x-h3-4x2g (Salvator-X board with H3 ES3.0 SoC installed)
- salvator-xs-h3  (Salvator-XS board with H3 ES2.0 SoC installed)
- salvator-xs-h3-2x2g (Salvator-XS board with H3 ES3.0 SoC installed)
- salvator-xs-h3-4x2g (Salvator-XS board with H3 ES3.0 SoC installed)


6. You are done. The artifacts of the build are located at workspace_base directory:

workspace_base/build/build/deploy/
├── dom0-image-thin-initramfs
│   └── images
│       └── generic-armv8-xt
│  
├── domd-image-weston
│   └── images
│       └──  MACHINE_NAME-xt
│  
├── domu-image-android
│   └── images
│       └── qemux86-64
│           ├── boot.img
│           ├── Image -> vmlinux
│           ├── system.img
│           ├── userdata.img
│           ├── vendor.img
│           └── vmlinux
└── mk_sdcard_image.sh

Images are located at:

- Domain-0:
workspace_base/build/build/deploy/dom0-image-thin-initramfs/images/generic-armv8-xt
Here we get a part of boot images:
  - uInitramfs - thin-initramfs for Domain-0
  - Image - Kernel image for Domain-0

- DomD:
workspace_base/build/build/deploy/domd-image-weston/images/MACHINE-NAME-xt
Here we get a part of boot images, all bootloader images and rootfs image for DomD:
  - xen-uImage - Xen main image
  - xenpolicy - special image for Xen usage
  - dom0.dtb - device-tree image for Domain-0
  - bootloader images in both binary and srec formats.
  - core-image-weston-MACHINE_NAME-xt.tar.bz2 - rootfs image for DomD

- DomA:
workspace_base/build/build/deploy/domu-image-android/images/qemux86-64
Here we get a rootfs image for DomU:
  - Image -> vmlinux - Kernel image for DomA
  - boot.img - android boot image(won't be used further)
  - system.img - android system image 
  - vendor.img - android vendor image
  - userdata.img - android vendor image

Build logs are located at:
- Domain-0:
workspace_base/build/build/log/dom0-image-thin-initramfs/cooker/generic-armv8-xt
- DomD:
workspace_base/build/tmp/log/domd-image-weston/cooker/MACHINE-NAME-xt
- DomA:
workspace_base/build/build/log/domu-image-android/cooker/qemux86-64

7. If one wants to build any domain's images by hand, at the time of development
for instance, it is possible by going into desired directory and using poky to build:

- For building Domain-0:
cd workspace_base/build/build/tmp/work/x86_64-xt-linux/dom0-image-thin-initramfs/1.0-r0/repo/

- For building DomD:
cd workspace_base/build/build/tmp/work/x86_64-xt-linux/domd-image-weston/1.0-r0/repo/

- For building DomA:
cd workspace_base/build/build/tmp/work/x86_64-xt-linux/domu-image-android/1.0-r0/repo/

source poky/oe-init-build-env

- For building Domain-0:
bitbake core-image-thin-initramfs

- For building DomD or DomA:
bitbake core-image-weston

- For building DomA:
bitbake android

Usage:
======

Different helpers scripts and docs are located at:
build-workspace/build/meta-xt-prod-devel/doc

Let's consider available boot options in details.

1. Using a storage device.
In order to boot system using a storage device, required storage device should be prepared
and flashed beforehand. The mk_sdcard_image.sh script is intended to help with that:

sudo ./mk_sdcard_image.sh -p /IMAGE_FOLDER -d /IMAGE_FILE -c devel

Where, IMAGE_FOLDER is a path to a folder where artifacts live (in the context of this document
it is a "deploy" directory) and IMAGE_FILE is an output image file or physical device how
it is appears in the filesystem (/dev/sdx). As far as script intended to support different products,
we need to specify '-c devel' for this product.

1.1 In case of SDx card booting we just have to insert SD card to a Host machine and run a script,
the latter will do all required actions automatically. All what we need to care about is to write
proper environment variables from U-Boot command line (boot_dev/bootcmd) according to the chosen SDx.
See workspace_base/meta-xt-prod-gen3-test/doc/u-boot-env-salvator-x.txt for details.

1.2 In case of eMMC booting, we have to have an access to it in order to flash images.
It is going to be not quite easy as for removable SD card, but the one of the possible ways is
to prepare the image blob using the same script, copy resulting blob to NFS root directory,
set system to boot via NFS, go to a DomD on target (where eMMC device is available)
and using "dd" command just copy blob to eMMC.

For example,

prepare an image blob:
sudo ./mk_sdcard_image.sh -p /IMAGE_FOLDER -d /home/emmc.img -c devel

and then run on target:
dd if=/home/emmc.img of=/dev/mmcblk0

After getting eMMC flashed we have to choose it to be an boot device in a similar way as it is done
for SD card.
See u-boot-env.txt for details.

So, as you can see, varying U-Boot's "boot_dev" and "bootcmd" environment variables
and domain config's "extra" and "disk" options it is possible to choose  different boot device
for each system component.

Additional script available in the product.
1. uirfs.sh.
This script is intended to pack/unpack uInitramfs for Domain-0. It might be helpful since uInitramfs
contains a lot of things which may changed during testing. The "xt" directory ships all guest domain
configs, device-tree and Kernel images, etc.

For example,

unpack uInitramfs:
cd /srv
sudo mkdir initramfs
sudo ./uirfs.sh unpack uInitramfs initramfs

Modify it's components if needed.
For example, domain config files located at:
/srv/initramfs/xt/dom.cfg/

pack it back:
sudo ./uirfs.sh pack uInitramfs initramfs

2. Product image structure

When you create image(or flash product to SD-card) using mk_sdcard_image.sh, volume structure of it will be
the following:

Device       Start      End Sectors  Size Id Type
/dev/sdX1     2048   526335  524288  256M 83 Linux  <---- Ext4 boot partition used by U-Boot to load xen,
                                                               Dom0 kernel, Dom0 initramfs
/dev/sdX2   526336  4622335 4096000    2G 83 Linux  <---- Ext4 partition used by DomD
/dev/sdX3  4622336 13680639 9058304  4,3G 83 Linux  <---- Virtualized block device for DomA

/dev/sdX3 is not a typical partition. It contains complete disk device for DomA. To access its internal partitions
you need to do the following:

sudo losetup -P -f /dev/sdX3

it will create new loop device(/dev/loopX), to retrive particular device name:

echo `sudo losetup -j /dev/sdX3 | cut -d":" -f1`

It will have following partitions:

Device         Start     End Sectors   Size Type
/dev/loopXp1    2048 6148095 6146048     3G Linux  <---- Ext4 android system partition
/dev/loopXp2 6150144 6676479  526336   257M Linux  <---- Ext4 android vendor partition
/dev/loopXp3 6678528 6680575    2048     1M Linux  <---- Raw android misc partition
/dev/loopXp4 6681640 8634765 1953126 953,7M Linux  <---- Ext4 Android userdata partition

To delete loop device:

sudo losetup -d /dev/loopX
